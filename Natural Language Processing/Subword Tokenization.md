## Premise

Subword-based tokenization is a solution between word and character-based tokenization. The main idea is to solve the issues faced by word-based tokenization (very large vocabulary size, large number of OOV tokens, and different meaning of very similar words) and character-based tokenization (very long sequences and less meaningful individual tokens). ğŸ˜ 

The subword-based tokenization algorithms do not split the frequently used words into smaller subwords. It rather splits the rare words into smaller meaningful subwords.

### Example
For example, â€œboyâ€ is not split but â€œboysâ€ is split into â€œboyâ€ and â€œsâ€. This helps the model learn that the word â€œboysâ€ is formed using the word â€œboyâ€ with slightly different meanings but the same root word.

## Byte-Pair Encoding(BPE)
BPE is a simple form of data compression algorithm in which the most common pair of consecutive bytes of data is replaced with a byte that does not occur in that data.
### Example of Basic Variant

| Word        | Most frequent Pair | Rules                  |
| ----------- | ------------------ | ---------------------- |
| aaabdaaabac | aa                 | -                      |
| ZabdZabac   | ab                 | **Z = aa**             |
| ZYdZYac     | **ZY**             | Z = aa, **Y=ab**       |
| XdXac       | -                  | Z = aa, Y=ab, **X=ZY** |
It cannot be further compressed as there are no byte pairs appearing more than once. We decompress the data by performing replacements in reverse order.

### NLP Variant
BPE ensures that the most common words are represented in the vocabulary as a single token while the rare words are broken down into two or more subword tokens and this is in agreement with what a subword-based tokenization algorithm does.

#### NLP Variant Example
Corpus: **{â€œoldâ€: 7, â€œolderâ€: 3, â€œfinestâ€: 9, â€œlowestâ€: 4}**

Let us add a special end token â€œ</w>â€ at the end of each word.

**{â€œold</w>â€: 7, â€œolder</w>â€: 3, â€œfinest</w>â€: 9, â€œlowest</w>â€: 4}**

The â€œ</w>â€ token at the end of each word is added to identify a word boundary so that the algorithm knows where each word ends. This helps the algorithm to look through each character and find the highest frequency character pairing.

Moving on next, we will split each word into characters and count their occurrence. The initial tokens will be all the characters and the â€œ</w>â€ token. Since we have 23 words in total, so we have 23 â€œ</w>â€ tokens. The second highest frequency token is â€œeâ€. In total, we have 12 different tokens

![[Pasted image 20240419150129.png | 500]]

##### Iterations

**Iteration 1**: The second most common token is â€eâ€, and the most common byte pair in our corpus with â€œeâ€ is â€œeâ€ and â€œsâ€ (in the words finest and lowest) which occurred 9 + 4 = 13 times. We merge them to form a new token â€œesâ€ and note down its frequency as 13. We will also reduce the count 13 from the individual tokens (â€œeâ€ and â€œsâ€)

![[Pasted image 20240419150556.png|500]]

**Iteration 2:** We will now merge the tokens â€œesâ€ and â€œtâ€ as they have appeared 13 times in our corpus. So, we have a new token â€œestâ€ with frequency 13 and we will reduce the frequency of â€œesâ€ and â€œtâ€ by 13.

![[Pasted image 20240419150637.png | 500]]

**Iteration 3:** Letâ€™s work now with the â€œ</w>â€ token. We see that byte pair â€œestâ€ and â€œ</w>â€ occurred 13 times in our corpus.

![[Pasted image 20240419150704.png|500]]

> Merging stop token </w> is very important. This helps the algorithm understand the difference between the words like â€œestimateâ€ and â€œhighestâ€. Both these words have â€œestâ€ in common but one has an â€œestâ€ token in the end and one at the start. Thus tokens like â€œestâ€ and â€œest</w>â€ would be handled differently. If the algorithm will see the token â€œest</w>â€ it will know that it is the token for the word â€œhighestâ€ and not for the word â€œestateâ€.

**Iteration 4**: Looking at the other tokens, we see that byte pairs â€œoâ€ and â€œlâ€ occurred 7 + 3 = 10 times in our corpus.
![[Pasted image 20240419151028.png | 500]]

**Iteration 5:** We now see that byte pairs â€œolâ€ and â€œdâ€ occurred 10 times in our corpus.
![[Pasted image 20240419151103.png | 500]]
If we now look at our table, we see that the frequency of â€œfâ€, â€œiâ€, and â€œnâ€ is 9 but we have just one word with these characters, so we are not merging them. For the sake of the simplicity of this article, let us now stop our iterations and closely look at our tokens.

##### Encoding and Decoding

Let us now see how we will decode our example. To decode, we have to simply concatenate all the tokens together to get the whole word. For example, the encoded sequence [â€œthe</w>â€, â€œhighâ€, â€œest</w>â€, â€œrange</w>â€, â€œin</w>â€, â€œSeattle</w>â€], we will be decoded as [â€œtheâ€, â€œhighestâ€, â€œrangeâ€, â€œinâ€, â€œSeattleâ€] and not as [â€œtheâ€, â€œhighâ€, â€œestrangeâ€, â€œinâ€, â€œSeattleâ€]. Notice the presence of the â€œ</w>â€ token in â€œestâ€.

## References
1. [Byte-Pair Encoding: Subword-based Tokenization - by Towards Data Science](https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0)
2. 