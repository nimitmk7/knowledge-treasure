ConsiderÂ $n$Â independent trials of an experiment where each trial is a "success" with probabilityÂ $p$. LetÂ $X$Â be the number of successes inÂ $n$Â trials. 

This situation is truly common in the natural world, and as such, there has been a lot of research into such phenomena. Random variables likeÂ $X$Â are called binomial random variables. If you can identify that a process fits this description, you can inherit many already proved properties such as the PMF formula, expectation, and variance!
![[Pasted image 20240519165552.png]]
Here are a few examples of binomial random variables:

-  # of heads inÂ ğ‘›Â coin flips
-  # of 1â€™s in randomly generated lengthÂ ğ‘›Â bit string
-  # of disk drives crashed in 1000 computer cluster, assuming disks crash independently.

## Formulation

**Notation**: $X \sim Bin(n,p)$
**Description**: Number of "successes" inÂ $n$Â identical, independent experiments each with probability of successÂ $p$.
**Parameters**:  $n \in \{0,1,â€¦\}$, the number of experiments, $p\in[0,1],$ the probability that a single experiment gives a â€œsuccessâ€
**Support**: $x \in \{ 0, 1, â€¦, n \}$
**PMF Equation**: $P(X=x) = \binom{n}{x}p^x(1-p)^{n-x}$
**Expectation**: $E[X] = n\cdot p$
**Variance**: $\text{Var}(X) = n \cdot p \cdot(1-p)$

## Intuition
One way to think of the binomial is as the sum ofÂ $n$Â [[Bernoulli Distribution]] variables. 

## PMF
![[Pasted image 20240519170536.png]]

## References
1. https://chrispiech.github.io/probabilityForComputerScientists/en/part2/binomial/